\subsection{Mobile optimisation overview}
In order to run deep learning models on mobile devices, it is necessary to understand the target platforms.
\citet{deng2019deep} investigated the support of the mobile hardware architecture and mobile operating system for deep learning, as well as the status quo of mobile deep learning frameworks, such as Tensorflow Lite, CoreML and etc.
Then he explained the model compression techniques such as algorithmic optimisation and joint software-hardware co-design.

In 2020, \citet{chen2020deep} conducted an in-depth survey of the important compression and acceleration techniques on mobile devices, and classified them into ``pruning, quantisation, model distillation, network design strategies, and low-rank factorisation.''
Pruning and quantisation are the two techniques for compressing model parameters to save storage, while model distillation, network design strategies are the two techniques for improving the inference speed of model.

In detail, pruning is a compression technique to trim unimportant weights in models, then achieve model sparsity to allow compression algorithms to become more effective and reduce the storage cost.
This compression technique may also bring about little speed increase, because some unnecessary residual paths may be short-circuited.
Another model compression technique is quantisation that directly lower the precision of the model parameters, e.g. use 16-bit float instead of 32-bit float.
The process can be carried out in the model deployment and conversion even after the training is completed.
These two compression technologies may bring uncertain and insignificant acceleration, so the improvement of model calculation performance mainly depends on model distillation and network design strategies.

The idea of model distillation is derived from transfer learning, which has a two-step training process, first pre-training the model on a large dataset and then fine-tuning the pre-training model on another smaller data set.
The model distillation process has similar ideas that transfer dark knowledge from the teacher network to train a smaller student network.
Then, the large teacher network is converted into a small network while retaining the performance close to the teacher network.
Although model distillation is a very effective optimisation and has quite a lot of successful researches in the NLP field, the implementation is more complicated, because transferred knowledge needs to be carefully defined according to different models.