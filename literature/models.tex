\section{Deep models detail}
\label{sec:Deep models detail}
% 7 pages
\subsection{Convolutional neural network}
\citet{fukushima1980neocognitron}

\citet{zhang1988shift}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{literature/imgs/ext-lecun-cnn-arch.png}
    \caption{CNN applications in vision by \citet{lecun2010convolutional}}
    \label{fig:ext-lecun-cnn-arch}
\end{figure}

\citet{lecun2010convolutional}

\citet{gu2018recent}

\subsection{Recurrent neural network}
\citet{jordan1997serial}

%LSTM
\citet{hochreiter1997long}

% RNN & LSTM fundamentals summary
\citet{sherstinsky2020fundamentals}

%GRU
\citet{chung2014empirical}

% et optimus
\subsection{Attention and Transformer}
%Attention
\citet{bahdanau2016neural}

\begin{minipage}[ht]{.6\textwidth}
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{literature/imgs/ext-transformer.png}
        \caption{The Transformer model architecture \cite{vaswani2017attention}}
        \label{fig:ext-transformer}
    \end{figure}
\end{minipage}
\begin{minipage}[ht]{.35\textwidth}
    \centering
    \begin{figure}[H]
        \centering
        \includegraphics[width=.48\textwidth]{literature/imgs/ext-attention-dot-product.png}
        \caption{Scaled Dot-Product Attention \cite{vaswani2017attention}}
        \label{fig:ext-attention-dot-product}
    \end{figure}
    \vspace*{-.5cm}
    \begin{figure}[H]
        \centering
        \includegraphics[width=.9\textwidth]{literature/imgs/ext-attention-multihead.png}
        \vspace*{-.4cm}
        \caption{Multi-Head Attention \cite{vaswani2017attention}}
        \label{fig:ext-attention-multihead}
    \end{figure}
\end{minipage}

\citet{vaswani2017attention}

%Transformer
\citet{devlin2019bert}

% \citet{lohit2019temporal}

\subsection{State-of-the-art models}

\citet{srinivas2021bottleneck}

\citet{wang2021pyramid}

\citet{dai2021coatnet}
